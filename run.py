"""
Llama 3.1 8B Instruct â€” Text Generation Demo
=============================================
Metaì˜ Llama 3.1 8B Instruct ëª¨ë¸ì„ ì‚¬ìš©í•œ í…ìŠ¤íŠ¸ ìƒì„± ë°ëª¨ì…ë‹ˆë‹¤.
AIë¥¼ ì²˜ìŒ ì ‘í•˜ëŠ” ë¶„ë„ ì‰½ê²Œ ì‹¤í–‰í•´ë³¼ ìˆ˜ ìˆë„ë¡ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
"""

import torch
import transformers

# =============================================
# ì„¤ì • (ì›í•˜ëŠ” ëŒ€ë¡œ ìˆ˜ì • ê°€ëŠ¥)
# =============================================

# ëª¨ë¸ ê²½ë¡œ (setup.sh ì‹¤í–‰ í›„ ë¡œì»¬ ê²½ë¡œ ì‚¬ìš©)
# setup.shë¥¼ ì‹¤í–‰í•˜ì§€ ì•Šì€ ê²½ìš° Hugging Face Hubì—ì„œ ìë™ ë‹¤ìš´ë¡œë“œ
MODEL_ID = "/workspace/models/llama-3.1-8b-instruct"
FALLBACK_MODEL_ID = "meta-llama/Meta-Llama-3.1-8B-Instruct"

# ì—¬ê¸°ì— ì›í•˜ëŠ” ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”!
USER_PROMPT = "ì¸ê³µì§€ëŠ¥ì´ë€ ë¬´ì—‡ì¸ê°€ìš”? ì‰½ê²Œ ì„¤ëª…í•´ ì£¼ì„¸ìš”."

# ìƒì„±í•  ìµœëŒ€ í† í° ìˆ˜ (ê¸¸ì´ ì¡°ì ˆ)
MAX_NEW_TOKENS = 512

# 4-bit ì–‘ìí™” ì‚¬ìš© ì—¬ë¶€ (VRAM 8GB í™˜ê²½ì—ì„œ Trueë¡œ ì„¤ì •)
USE_4BIT = False

# =============================================


def load_model(model_id: str, use_4bit: bool):
    """ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    import os
    # ë¡œì»¬ ê²½ë¡œì— ëª¨ë¸ì´ ì—†ìœ¼ë©´ Hugging Face Hubì—ì„œ ë‹¤ìš´ë¡œë“œ
    if not os.path.exists(model_id):
        print(f"âš ï¸  ë¡œì»¬ ëª¨ë¸ ê²½ë¡œ({model_id})ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print(f"   Hugging Face Hubì—ì„œ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤: {FALLBACK_MODEL_ID}")
        model_id = FALLBACK_MODEL_ID
    print(f"ëª¨ë¸ ë¡œë”© ì¤‘... ({model_id})")

    model_kwargs = {
        "torch_dtype": torch.bfloat16,
        "device_map": "auto",
    }

    # 4-bit ì–‘ìí™” ì„¤ì • (VRAM ë¶€ì¡± ì‹œ í™œì„±í™”)
    if use_4bit:
        from transformers import BitsAndBytesConfig
        model_kwargs["quantization_config"] = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.bfloat16,
        )
        print("  â„¹ï¸  4-bit ì–‘ìí™” ëª¨ë“œë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤ (VRAM ì ˆì•½)")

    pipeline = transformers.pipeline(
        "text-generation",
        model=model_id,
        model_kwargs=model_kwargs,
    )

    print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ!\n")
    return pipeline


def generate_response(pipeline, user_prompt: str, max_new_tokens: int) -> str:
    """ëª¨ë¸ì— ì§ˆë¬¸ì„ ì „ë‹¬í•˜ê³  ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤."""
    messages = [
        {
            "role": "system",
            "content": "You are a helpful AI assistant. Please respond in Korean.",
        },
        {
            "role": "user",
            "content": user_prompt,
        },
    ]

    outputs = pipeline(
        messages,
        max_new_tokens=max_new_tokens,
        do_sample=True,
        temperature=0.7,
        top_p=0.9,
    )

    # ë§ˆì§€ë§‰ ë©”ì‹œì§€(assistantì˜ ì‘ë‹µ) ì¶”ì¶œ
    response = outputs[0]["generated_text"][-1]["content"]
    return response


def main():
    print("=" * 50)
    print("  ğŸ¦™ Llama 3.1 8B Instruct â€” Demo")
    print("=" * 50)

    # GPU ìƒíƒœ í™•ì¸
    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name(0)
        vram = torch.cuda.get_device_properties(0).total_memory / 1024**3
        print(f"ğŸ–¥ï¸  GPU: {gpu_name} ({vram:.1f}GB VRAM)")
    else:
        print("âš ï¸  GPUë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPUë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤ (ë§¤ìš° ëŠë¦´ ìˆ˜ ìˆìŒ)")

    print()

    # ëª¨ë¸ ë¡œë“œ
    pipeline = load_model(MODEL_ID, USE_4BIT)

    # ì‘ë‹µ ìƒì„±
    print(f"[ì§ˆë¬¸] {USER_PROMPT}\n")
    print("[ë‹µë³€]")
    print("-" * 50)

    response = generate_response(pipeline, USER_PROMPT, MAX_NEW_TOKENS)
    print(response)

    print("-" * 50)
    print("\nâœ… ì™„ë£Œ! run.pyì˜ USER_PROMPTë¥¼ ìˆ˜ì •í•´ì„œ ë‹¤ë¥¸ ì§ˆë¬¸ì„ í•´ë³´ì„¸ìš”.")


if __name__ == "__main__":
    main()